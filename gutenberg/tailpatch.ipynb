{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12052eb4",
   "metadata": {},
   "source": [
    "## Tailpatch for Gutenberg experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TailPatch Function: With Log Likelihood\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import math\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def tp_debug_log(samples,output_ids, prompt_length=1):\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        samples,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    device = 'cpu'\n",
    "    model = GPT2LMHeadModel.from_pretrained('out/gpt2-scratch-mixed')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    with torch.no_grad():\n",
    "        model_pass = model(output_ids, labels=output_ids)\n",
    "\n",
    "    logits = model_pass.logits\n",
    "    logits = logits[:, prompt_length-1:-1, :]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    target_ids = output_ids[:, prompt_length:]\n",
    "    token_probs = log_probs.gather(2, target_ids.unsqueeze(-1))\n",
    "    token_probs = token_probs.squeeze(-1)\n",
    "    print(tokenizer.decode(target_ids[0], skip_special_tokens=True))\n",
    "    original_probability = 0\n",
    "    for i in range(token_probs.shape[1]):\n",
    "        original_probability += token_probs[0,i]\n",
    "    original_probability = math.exp(original_probability.item())\n",
    "    print('Original Probability:', original_probability)\n",
    "    original_probs = token_probs.clone()\n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        del input_ids, attention_mask, outputs, labels, loss\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_pass = model(output_ids, labels=output_ids)\n",
    "\n",
    "    logits = model_pass.logits\n",
    "    logits = logits[:, prompt_length-1:-1, :]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    target_ids = output_ids[:, prompt_length:]\n",
    "    token_probs = log_probs.gather(2, target_ids.unsqueeze(-1))\n",
    "    token_probs = token_probs.squeeze(-1)\n",
    "    probability = 1\n",
    "    for i in range(token_probs.shape[1]):\n",
    "        probability += token_probs[0,i]\n",
    "    probability = math.exp(probability.item())   \n",
    "    return probability, original_probability, token_probs, original_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Model: gpt2-scratch-mixed\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"out/gpt2-scratch-mixed\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('selected_dataset_mixed.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "training_data = data['train_data_np']\n",
    "training_data_authors = data['train_data_authors']\n",
    "training_data_titles = data['train_data_titles']\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ddbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# List and sort checkpoint files\n",
    "## As a proxy, to get the target sequence which is stored along with the gradient descent optimized model. could also use the unlearned model. \n",
    "ckpt_dir = 'out/gutenberg_experiments/finetuned_models'\n",
    "ckpt_files = sorted(os.listdir(ckpt_dir))\n",
    "print(\"Checkpoint files:\", ckpt_files)\n",
    "\n",
    "# List and sort finetuned loss files\n",
    "finetuned_dir = 'data/losses/gutenberg/experiments_finetuned'\n",
    "finetuned_losses = sorted(os.listdir(finetuned_dir))\n",
    "print(\"Finetuned loss files:\", finetuned_losses)\n",
    "\n",
    "# List and sort unlearned loss files\n",
    "unlearned_dir = 'data/losses/gutenberg/experiments_unlearned'\n",
    "unlearned_losses = sorted(os.listdir(unlearned_dir))\n",
    "print(\"Unlearned loss files:\", unlearned_losses)\n",
    "assert len(finetuned_losses) == len(unlearned_losses)\n",
    "assert len(ckpt_files) == len(finetuned_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import json\n",
    "with open('selected_dataset_mixed_decoded.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents.\")\n",
    "print(\"Sample:\", corpus[0][:300])\n",
    "corpus = np.array(corpus)\n",
    "\n",
    "print(len(corpus))\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5860c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfs = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for k, file_path in enumerate(ckpt_files):\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, file_path), map_location=device)\n",
    "    output_ids = ckpt['output_ids']\n",
    "    prompt_length = ckpt['prompt_length']\n",
    "    losses_ft = np.load(os.path.join(finetuned_dir, finetuned_losses[k]))\n",
    "    losses_unl = np.load(os.path.join(unlearned_dir, unlearned_losses[k]))\n",
    "    losses_diff = np.abs(losses_ft - losses_unl)\n",
    "    sorted_indices = np.argsort(losses_diff)[::-1]\n",
    "    print(tokenizer.decode(ckpt['output_ids'][0]))\n",
    "    samples = []\n",
    "    for i in range(20):\n",
    "        samples.append({\n",
    "            'input_ids': torch.tensor([training_data[int(sorted_indices[i])]]),\n",
    "            'attention_mask': torch.tensor([1] * len(training_data[int(sorted_indices[i])])),\n",
    "            'labels': torch.tensor([training_data[int(sorted_indices[i])]])\n",
    "        })\n",
    "    p_1, original, _, _ = tp_debug_log(samples[:1], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_3, _, _, _ = tp_debug_log(samples[:3], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_5, _, _, _ = tp_debug_log(samples[:5], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_7, _, _, _ = tp_debug_log(samples[:7], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_10, _, _, _ = tp_debug_log(samples[:10], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_15, _, _, _ = tp_debug_log(samples[:15], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_20, _, _, _ = tp_debug_log(samples[:20], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'FT & UN',\n",
    "        'p_1': p_1,\n",
    "        'p_3': p_3,\n",
    "        'p_5': p_5,\n",
    "        'p_7': p_7,\n",
    "        'p_10': p_10,\n",
    "        'p_15': p_15,\n",
    "        'p_20': p_20,\n",
    "        \n",
    "    })\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'Original',\n",
    "        'p_1': original,\n",
    "        'p_3': original,\n",
    "        'p_5': original,\n",
    "        'p_7': original,\n",
    "        'p_10': original,\n",
    "        'p_15': original,\n",
    "        'p_20': original,\n",
    "    })\n",
    "    random_samples_ = np.random.choice(len(training_data), size=20)\n",
    "    random_samples = []\n",
    "    for i in range(20):\n",
    "        random_samples.append({\n",
    "            'input_ids': torch.tensor(training_data[random_samples_[i]]),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(training_data[random_samples_[i]])),\n",
    "            'labels': torch.tensor(training_data[random_samples_[i]])\n",
    "        })\n",
    "    p1, original,_ ,_ = tp_debug_log(random_samples[:1], output_ids, prompt_length)\n",
    "    p3, original,_ ,_ = tp_debug_log(random_samples[:3], output_ids, prompt_length)\n",
    "    p5, original,_ ,_ = tp_debug_log(random_samples[:5], output_ids, prompt_length)\n",
    "    p7, original,_ ,_ = tp_debug_log(random_samples[:7], output_ids, prompt_length)\n",
    "    p10, original,_ ,_ = tp_debug_log(random_samples[:10], output_ids, prompt_length)\n",
    "    p15, original,_ ,_ = tp_debug_log(random_samples[:15], output_ids, prompt_length)\n",
    "    p20, original,_ ,_ = tp_debug_log(random_samples[:20], output_ids, prompt_length)\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'Random',\n",
    "        'p_1': p1,\n",
    "        'p_3': p3,\n",
    "        'p_5': p5,\n",
    "        'p_7': p7,\n",
    "        'p_10': p10,\n",
    "        'p_15': p15,\n",
    "        'p_20': p20\n",
    "    })\n",
    "    trackstar_influence = np.load(f'data/trackstar/gutenberg/testing/gradients/influence_list_{file_path[:-3]}.npy')\n",
    "    print(tokenizer.decode(ckpt['output_ids'][0]))\n",
    "    sorted_indices = np.argsort(trackstar_influence)[::-1]\n",
    "    samples = []\n",
    "    for i in range(20):\n",
    "        samples.append({\n",
    "            'input_ids': torch.tensor([training_data[int(sorted_indices[i])]]),\n",
    "            'attention_mask': torch.tensor([1] * len(training_data[int(sorted_indices[i])])),\n",
    "            'labels': torch.tensor([training_data[int(sorted_indices[i])]])\n",
    "        })\n",
    "    p_1, original, _, _ = tp_debug_log(samples[:1], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_3, _, _, _ = tp_debug_log(samples[:3], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_5, _, _, _ = tp_debug_log(samples[:5], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_7, _, _, _ = tp_debug_log(samples[:7], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_10, _, _, _ = tp_debug_log(samples[:10], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_15, _, _, _ = tp_debug_log(samples[:15], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_20, _, _, _ = tp_debug_log(samples[:20], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'TrackStar',\n",
    "        'p_1': p_1,\n",
    "        'p_3': p_3,\n",
    "        'p_5': p_5,\n",
    "        'p_7': p_7,\n",
    "        'p_10': p_10,\n",
    "        'p_15': p_15,\n",
    "        'p_20': p_20,\n",
    "        \n",
    "    })\n",
    "    output_ids = ckpt['output_ids']\n",
    "    prompt_length = ckpt['prompt_length']\n",
    "    \n",
    "    query = tokenizer.decode(output_ids[0, prompt_length:], skip_special_tokens=True)\n",
    "    print(query)\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "    samples_bm25 = []\n",
    "    for rank, (idx, score) in enumerate(top_n, 1):\n",
    "        print(f\"\\nTop {rank} (index: {idx}, score: {score:.2f}):\\n{corpus[idx][:300]}\")\n",
    "        print(tokenizer.decode(training_data[idx], skip_special_tokens=True))\n",
    "        sample_bm25 = {\n",
    "            'input_ids': torch.tensor(training_data[idx]),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(training_data[idx])),\n",
    "            'labels': torch.tensor(training_data[idx])\n",
    "        }\n",
    "        samples_bm25.append(sample_bm25)\n",
    "\n",
    "    p_1, original, _, _ = tp_debug_log(samples_bm25[:1],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_3, original, _, _ = tp_debug_log(samples_bm25[:3],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_5, original, _, _ = tp_debug_log(samples_bm25[:5],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_7, original, _, _ = tp_debug_log(samples_bm25[:7],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_10, original, _, _ = tp_debug_log(samples_bm25[:10],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_15, original, _, _ = tp_debug_log(samples_bm25[:15],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_20, original, _, _ = tp_debug_log(samples_bm25[:20],output_ids=output_ids, prompt_length=prompt_length)\n",
    "\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'BM25',\n",
    "        '1': p_1,\n",
    "        '3': p_3,\n",
    "        '5': p_5,\n",
    "        '7': p_7,\n",
    "        '10': p_10,\n",
    "        '15': p_15,\n",
    "        '20': p_20\n",
    "    })\n",
    "    \n",
    "    total_df = pd.DataFrame(dfs)\n",
    "    os.makedirs('data/results', exist_ok=True)\n",
    "    total_df.to_csv('data/results/gutenberg_tailpatch_results.csv', index=False)\n",
    "    \n",
    "\n",
    "total_df = pd.DataFrame(dfs)\n",
    "total_df.to_csv('data/results/gutenberg_tailpatch_results.csv', index=False)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36245df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfs = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for k, file_path in enumerate(ckpt_files):\n",
    "    ckpt = torch.load(os.path.join(ckpt_dir, file_path), map_location=device)\n",
    "    output_ids = ckpt['output_ids']\n",
    "    prompt_length = ckpt['prompt_length']\n",
    "    losses_ft = np.load(os.path.join(finetuned_dir, finetuned_losses[k]))\n",
    "    losses_unl = np.load(os.path.join(unlearned_dir, unlearned_losses[k]))\n",
    "    losses_diff = np.abs(losses_ft - losses_unl)\n",
    "    sorted_indices = np.argsort(losses_diff)[::-1]\n",
    "    print(tokenizer.decode(ckpt['output_ids'][0]))\n",
    "    samples = []\n",
    "    for i in range(20):\n",
    "        samples.append({\n",
    "            'input_ids': torch.tensor([training_data[int(sorted_indices[i])]]),\n",
    "            'attention_mask': torch.tensor([1] * len(training_data[int(sorted_indices[i])])),\n",
    "            'labels': torch.tensor([training_data[int(sorted_indices[i])]])\n",
    "        })\n",
    "    p_1, original, _, _ = tp_debug_log(samples[:1], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_3, _, _, _ = tp_debug_log(samples[:3], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_5, _, _, _ = tp_debug_log(samples[:5], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_7, _, _, _ = tp_debug_log(samples[:7], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_10, _, _, _ = tp_debug_log(samples[:10], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_15, _, _, _ = tp_debug_log(samples[:15], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_20, _, _, _ = tp_debug_log(samples[:20], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'FT & UN',\n",
    "        'p_1': p_1,\n",
    "        'p_3': p_3,\n",
    "        'p_5': p_5,\n",
    "        'p_7': p_7,\n",
    "        'p_10': p_10,\n",
    "        'p_15': p_15,\n",
    "        'p_20': p_20,\n",
    "        \n",
    "    })\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'Original',\n",
    "        'p_1': original,\n",
    "        'p_3': original,\n",
    "        'p_5': original,\n",
    "        'p_7': original,\n",
    "        'p_10': original,\n",
    "        'p_15': original,\n",
    "        'p_20': original,\n",
    "    })\n",
    "    random_samples_ = np.random.choice(len(training_data), size=20)\n",
    "    random_samples = []\n",
    "    for i in range(20):\n",
    "        random_samples.append({\n",
    "            'input_ids': torch.tensor(training_data[random_samples_[i]]),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(training_data[random_samples_[i]])),\n",
    "            'labels': torch.tensor(training_data[random_samples_[i]])\n",
    "        })\n",
    "    p1, original,_ ,_ = tp_debug_log(random_samples[:1], output_ids, prompt_length)\n",
    "    p3, original,_ ,_ = tp_debug_log(random_samples[:3], output_ids, prompt_length)\n",
    "    p5, original,_ ,_ = tp_debug_log(random_samples[:5], output_ids, prompt_length)\n",
    "    p7, original,_ ,_ = tp_debug_log(random_samples[:7], output_ids, prompt_length)\n",
    "    p10, original,_ ,_ = tp_debug_log(random_samples[:10], output_ids, prompt_length)\n",
    "    p15, original,_ ,_ = tp_debug_log(random_samples[:15], output_ids, prompt_length)\n",
    "    p20, original,_ ,_ = tp_debug_log(random_samples[:20], output_ids, prompt_length)\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'Random',\n",
    "        'p_1': p1,\n",
    "        'p_3': p3,\n",
    "        'p_5': p5,\n",
    "        'p_7': p7,\n",
    "        'p_10': p10,\n",
    "        'p_15': p15,\n",
    "        'p_20': p20\n",
    "    })\n",
    "    trackstar_influence = np.load(f'data/training_data/trackstar/gutenberg/testing/gradients/influence_list_{file_path[:-3]}.npy')\n",
    "    print(tokenizer.decode(ckpt['output_ids'][0]))\n",
    "    sorted_indices = np.argsort(trackstar_influence)[::-1]\n",
    "    samples = []\n",
    "    for i in range(20):\n",
    "        samples.append({\n",
    "            'input_ids': torch.tensor([training_data[int(sorted_indices[i])]]),\n",
    "            'attention_mask': torch.tensor([1] * len(training_data[int(sorted_indices[i])])),\n",
    "            'labels': torch.tensor([training_data[int(sorted_indices[i])]])\n",
    "        })\n",
    "    p_1, original, _, _ = tp_debug_log(samples[:1], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_3, _, _, _ = tp_debug_log(samples[:3], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_5, _, _, _ = tp_debug_log(samples[:5], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_7, _, _, _ = tp_debug_log(samples[:7], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_10, _, _, _ = tp_debug_log(samples[:10], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_15, _, _, _ = tp_debug_log(samples[:15], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    p_20, _, _, _ = tp_debug_log(samples[:20], output_ids, prompt_length=ckpt['prompt_length'])\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'TrackStar',\n",
    "        'p_1': p_1,\n",
    "        'p_3': p_3,\n",
    "        'p_5': p_5,\n",
    "        'p_7': p_7,\n",
    "        'p_10': p_10,\n",
    "        'p_15': p_15,\n",
    "        'p_20': p_20,\n",
    "        \n",
    "    })\n",
    "    output_ids = ckpt['output_ids']\n",
    "    prompt_length = ckpt['prompt_length']\n",
    "    \n",
    "    query = tokenizer.decode(output_ids[0, prompt_length:], skip_special_tokens=True)\n",
    "    print(query)\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "    samples_bm25 = []\n",
    "    for rank, (idx, score) in enumerate(top_n, 1):\n",
    "        print(f\"\\nTop {rank} (index: {idx}, score: {score:.2f}):\\n{corpus[idx][:300]}\")\n",
    "        print(tokenizer.decode(training_data[idx], skip_special_tokens=True))\n",
    "        sample_bm25 = {\n",
    "            'input_ids': torch.tensor(training_data[idx]),\n",
    "            'attention_mask': torch.ones_like(torch.tensor(training_data[idx])),\n",
    "            'labels': torch.tensor(training_data[idx])\n",
    "        }\n",
    "        samples_bm25.append(sample_bm25)\n",
    "\n",
    "    p_1, original, _, _ = tp_debug_log(samples_bm25[:1],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_3, original, _, _ = tp_debug_log(samples_bm25[:3],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_5, original, _, _ = tp_debug_log(samples_bm25[:5],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_7, original, _, _ = tp_debug_log(samples_bm25[:7],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_10, original, _, _ = tp_debug_log(samples_bm25[:10],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_15, original, _, _ = tp_debug_log(samples_bm25[:15],output_ids=output_ids, prompt_length=prompt_length)\n",
    "    p_20, original, _, _ = tp_debug_log(samples_bm25[:20],output_ids=output_ids, prompt_length=prompt_length)\n",
    "\n",
    "    dfs.append({\n",
    "        'name': file_path,\n",
    "        'method': 'BM25',\n",
    "        '1': p_1,\n",
    "        '3': p_3,\n",
    "        '5': p_5,\n",
    "        '7': p_7,\n",
    "        '10': p_10,\n",
    "        '15': p_15,\n",
    "        '20': p_20\n",
    "    })\n",
    "    \n",
    "    total_df = pd.DataFrame(dfs)\n",
    "    os.makedirs('data/results', exist_ok=True)\n",
    "    total_df.to_csv('data/results/gutenberg_tailpatch_results.csv', index=False)\n",
    "    \n",
    "\n",
    "total_df = pd.DataFrame(dfs)\n",
    "total_df.to_csv('data/results/gutenberg_tailpatch_results.csv', index=False)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/results/gutenberg_tailpatch_results_all.csv')\n",
    "df_exp = df.copy()\n",
    "df_exp[df_exp.select_dtypes(include=[np.number]).columns] = \\\n",
    "    df_exp.select_dtypes(include=[np.number]).applymap(lambda x: np.exp(x-1))\n",
    "\n",
    "df = df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cols_to_check = ['1', '3', '5', '7', '10', '15', '20']\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "results = []\n",
    "\n",
    "for namegrouped, group in df.groupby('name'):\n",
    "    original_row = group[group['method'].str.lower() == 'original']\n",
    "    if original_row.empty:\n",
    "        print(namegrouped)\n",
    "        continue\n",
    "    original_values = original_row[cols_to_check].iloc[0]\n",
    "    \n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        if row['method'].lower() == 'original':\n",
    "            continue\n",
    "        rel_increase = np.abs((row[cols_to_check].astype(float) - original_values.astype(float)) /np.abs(original_values.astype(float))) * 100\n",
    "        result_row = {\n",
    "            'name': namegrouped,\n",
    "            'method': row['method']\n",
    "        }\n",
    "        for col in cols_to_check:\n",
    "            result_row[f'rel_increase_{col}'] = rel_increase[col]\n",
    "        results.append(result_row)\n",
    "\n",
    "# Final result\n",
    "rel_df = pd.DataFrame(results)\n",
    "\n",
    "rel_df.loc[rel_df['name']=='William Shakespeare_2.pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = (\n",
    "    rel_df.groupby('method')\n",
    "    .agg({\n",
    "        'rel_increase_1': ['mean', 'std'],\n",
    "        'rel_increase_3': ['mean', 'std'],\n",
    "        'rel_increase_5': ['mean', 'std'],\n",
    "        'rel_increase_7': ['mean', 'std'],\n",
    "        'rel_increase_10': ['mean', 'std'],\n",
    "        'rel_increase_15': ['mean', 'std'],\n",
    "        'rel_increase_20': ['mean', 'std'],\n",
    "    })\n",
    ")\n",
    "avg_df = avg_df.round(2)\n",
    "avg_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "columns = ['rel_increase_1', 'rel_increase_3', 'rel_increase_5', \n",
    "           'rel_increase_7', 'rel_increase_10', 'rel_increase_15', 'rel_increase_20']\n",
    "\n",
    "for col in columns:\n",
    "    ft_values = rel_df[rel_df['method'] == 'FT & UN'][col]\n",
    "    trackstar_values = rel_df[rel_df['method'] == 'TrackStar'][col]\n",
    "    \n",
    "    t_stat, p_val = ttest_ind(ft_values, trackstar_values, equal_var=False)\n",
    "    print(f\"{col}: t = {t_stat:.3f}, p = {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ad0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_gecko = pd.read_csv('../data/results/results_aggregated_gecko_gutenberg.csv')\n",
    "df_gecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cols_to_check = ['1', '3', '5', '7', '10', '15', '20']\n",
    "df = df_gecko.copy()\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for namegrouped, group in df.groupby('name'):\n",
    "    original_row = group[group['method'].str.lower() == 'original']\n",
    "    if original_row.empty:\n",
    "        print(namegrouped)\n",
    "        continue\n",
    "    original_values = original_row[cols_to_check].iloc[0]\n",
    "    \n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        if row['method'].lower() == 'original':\n",
    "            continue\n",
    "        rel_increase = np.abs((row[cols_to_check].astype(float) - original_values.astype(float)) /np.abs(original_values.astype(float))) * 100\n",
    "        result_row = {\n",
    "            'name': namegrouped,\n",
    "            'method': row['method']\n",
    "        }\n",
    "        for col in cols_to_check:\n",
    "            result_row[f'rel_increase_{col}'] = rel_increase[col]\n",
    "        results.append(result_row)\n",
    "\n",
    "# Final result\n",
    "rel_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff856945",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df_gecko = (\n",
    "    rel_df.groupby('method')\n",
    "    .agg({\n",
    "        'rel_increase_1': ['mean', 'std'],\n",
    "        'rel_increase_3': ['mean', 'std'],\n",
    "        'rel_increase_5': ['mean', 'std'],\n",
    "        'rel_increase_7': ['mean', 'std'],\n",
    "        'rel_increase_10': ['mean', 'std'],\n",
    "        'rel_increase_15': ['mean', 'std'],\n",
    "        'rel_increase_20': ['mean', 'std'],\n",
    "    })\n",
    ")\n",
    "avg_df_gecko = avg_df_gecko.round(2)\n",
    "avg_df_gecko\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([avg_df_gecko, avg_df])\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_cleaned = combined_df.loc[:, combined_df.columns.get_level_values(1) == 'mean']\n",
    "df_cleaned.columns = [col[0].split('_')[-1] for col in df_cleaned.columns]\n",
    "\n",
    "df_cleaned.columns.name = None\n",
    "df_cleaned.index.name = None\n",
    "df_cleaned = df_cleaned.sort_values(by=['1'], ascending=True)\n",
    "df_cleaned.rename(index={'FT/UN': 'FT & UN'}, inplace=True)\n",
    "#df_cleaned.to_csv('avg_df_fisher_raw_.csv', index=True)\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('../data/results/avg_df_gutenberg_gecko.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"Retraining Gutenberg FTUN\")\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "\n",
    "history_df = {}\n",
    "authors = [\"Arthur Christopher Benson_2\", \"Carolyn Wells_2\",\n",
    "             \"Jane Austen_2\", \"Edward Payson Roe_2\" ]\n",
    "for run in runs: \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "          if not k.startswith('_')})  \n",
    "    name_list.append(run.name)\n",
    "    \n",
    "    config = run.config\n",
    "    author = config.get(\"author\", \"N/A\")\n",
    "    if author not in authors:\n",
    "        continue\n",
    "    num_samples = config.get(\"num_samples\", \"N/A\")\n",
    "    mode = config.get(\"mode\", \"N/A\")\n",
    "    if len(run.history()) > 20:\n",
    "        print(run.name)\n",
    "        print(f\"Run: {run.name}\")\n",
    "        print(f\"Author: {author}, Num Samples: {num_samples}, Mode: {mode}\")\n",
    "        history_df[(author, num_samples, mode)] = run.history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_for_bm25_50 = []\n",
    "losses_for_bm25_100 = []\n",
    "losses_for_bm25_20 = []\n",
    "losses_for_attributed_50 = []\n",
    "losses_for_attributed_100 = []\n",
    "losses_for_attributed_20 = []\n",
    "losses_for_trackstar_20 = []\n",
    "losses_for_trackstar_50 = []\n",
    "losses_for_trackstar_100 = []\n",
    "losses_for_gecko_20 = []\n",
    "losses_for_gecko_50 = []\n",
    "losses_for_gecko_100 = []\n",
    "for run in history_df.keys():\n",
    "    if run[2] == 'bm25':\n",
    "        masked_loss = history_df[run]\n",
    "        last_loss = masked_loss[\"Masked Loss\"].dropna().iloc[-1]\n",
    "        if run[1] == 50:\n",
    "            losses_for_bm25_50.append(last_loss)\n",
    "        elif run[1] == 100:\n",
    "            losses_for_bm25_100.append(last_loss)\n",
    "        elif run[1] == 20:\n",
    "            losses_for_bm25_20.append(last_loss)\n",
    "    if run[2] == 'attributed':\n",
    "        masked_loss = history_df[run]\n",
    "        last_loss = masked_loss[\"Masked Loss\"].dropna().iloc[-1]\n",
    "        if run[1] == 50:\n",
    "            losses_for_attributed_50.append(last_loss)\n",
    "        elif run[1] == 100:\n",
    "            losses_for_attributed_100.append(last_loss)\n",
    "        elif run[1] == 20:\n",
    "            losses_for_attributed_20.append(last_loss)\n",
    "    if run[2] == 'trackstar':\n",
    "        masked_loss = history_df[run]\n",
    "        last_loss = masked_loss[\"Masked Loss\"].dropna().iloc[-1]\n",
    "        if run[1] == 50:\n",
    "            losses_for_trackstar_50.append(last_loss)\n",
    "        elif run[1] == 100:\n",
    "            losses_for_trackstar_100.append(last_loss)\n",
    "        elif run[1] == 20:\n",
    "            losses_for_trackstar_20.append(last_loss)\n",
    "\n",
    "    if run[2] == 'gecko':\n",
    "        masked_loss = history_df[run]\n",
    "        last_loss = masked_loss[\"Masked Loss\"].dropna().iloc[-1]\n",
    "        if run[1] == 50:\n",
    "            losses_for_gecko_50.append(last_loss)\n",
    "        elif run[1] == 100:\n",
    "            losses_for_gecko_100.append(last_loss)\n",
    "        elif run[1] == 20:\n",
    "            losses_for_gecko_20.append(last_loss)\n",
    "\n",
    "print(len(losses_for_bm25_50))\n",
    "print(len(losses_for_bm25_100))\n",
    "print(len(losses_for_bm25_20))\n",
    "print(len(losses_for_gecko_50))\n",
    "print(losses_for_bm25_50)\n",
    "print(losses_for_bm25_100)\n",
    "print(losses_for_bm25_20)\n",
    "print(losses_for_trackstar_20)\n",
    "print(losses_for_trackstar_50)\n",
    "print(losses_for_trackstar_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee04172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "avgs_bm25 = [np.mean(np.array(losses_for_bm25_20)), np.mean(np.array(losses_for_bm25_50)), np.mean(np.array(losses_for_bm25_100))]\n",
    "avgs_attributed = [np.mean(np.array(losses_for_attributed_20)), np.mean(np.array(losses_for_attributed_50)), np.mean(np.array(losses_for_attributed_100))]\n",
    "avgs_trackstar = [np.mean(np.array(losses_for_trackstar_20)), np.mean(np.array(losses_for_trackstar_50)), np.mean(np.array(losses_for_trackstar_100))]\n",
    "avgs_gecko = [np.mean(np.array(losses_for_gecko_20)), np.mean(np.array(losses_for_gecko_50)), np.mean(np.array(losses_for_gecko_100))]\n",
    "print(avgs_bm25)\n",
    "print(avgs_attributed)\n",
    "print(avgs_trackstar)\n",
    "print(avgs_gecko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52617c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Averages\n",
    "avgs_bm25 = [np.mean(np.array(losses_for_bm25_20)), np.mean(np.array(losses_for_bm25_50)), np.mean(np.array(losses_for_bm25_100))]\n",
    "avgs_attributed = [np.mean(np.array(losses_for_attributed_20)), np.mean(np.array(losses_for_attributed_50)), np.mean(np.array(losses_for_attributed_100))]\n",
    "avgs_trackstar = [np.mean(np.array(losses_for_trackstar_20)), np.mean(np.array(losses_for_trackstar_50)), np.mean(np.array(losses_for_trackstar_100))]\n",
    "\n",
    "x_vals = [20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_losses = []\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('../out/gpt2-scratch-mixed')\n",
    "model.eval()\n",
    "for author in authors:\n",
    "    ckpt = torch.load(f'samples/{author}.pt', map_location='cpu')\n",
    "    sentence = ckpt['prompt'] + ckpt['sentence']\n",
    "    output_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    prompt_length = len(tokenizer.encode(ckpt['prompt']))\n",
    "    print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "    labels = output_ids.clone()\n",
    "    labels[..., :prompt_length] = -100\n",
    "    attention_mask = torch.ones_like(output_ids)\n",
    "    outputs = model(output_ids, labels=labels, attention_mask=attention_mask)\n",
    "    loss = outputs.loss\n",
    "    original_losses.append(loss.item())\n",
    "    print(loss.item())\n",
    "print(original_losses)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(original_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Averages\n",
    "avgs_bm25 = [np.mean(original_losses)] + [\n",
    "    np.mean(np.array(losses_for_bm25_20)),\n",
    "    np.mean(np.array(losses_for_bm25_50)),\n",
    "    np.mean(np.array(losses_for_bm25_100)),\n",
    "]\n",
    "avgs_attributed = [np.mean(original_losses)] + [\n",
    "    np.mean(np.array(losses_for_attributed_20)),\n",
    "    np.mean(np.array(losses_for_attributed_50)),\n",
    "    np.mean(np.array(losses_for_attributed_100)),\n",
    "]\n",
    "avgs_trackstar = [np.mean(original_losses)] + [\n",
    "    np.mean(np.array(losses_for_trackstar_20)),\n",
    "    np.mean(np.array(losses_for_trackstar_50)),\n",
    "    np.mean(np.array(losses_for_trackstar_100)),\n",
    "]\n",
    "avgs_gecko = [np.mean(original_losses)] + [\n",
    "    np.mean(np.array(losses_for_gecko_20)),\n",
    "    np.mean(np.array(losses_for_gecko_50)),\n",
    "    np.mean(np.array(losses_for_gecko_100)),\n",
    "]\n",
    "# X-axis values (without initial dummy 0)\n",
    "x_vals = [0,20, 50, 100]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(13, 7))  # was (8, 5) before â€” increase height\n",
    "\n",
    "plt.plot(x_vals, avgs_attributed, marker='o', label='Attributed')\n",
    "plt.plot(x_vals, avgs_bm25, marker='s', label='BM25')\n",
    "plt.plot(x_vals, avgs_trackstar, marker='^', label='Trackstar')\n",
    "plt.plot(x_vals, avgs_gecko, marker='^', label='Gecko')\n",
    "plt.xlim(0.5, 120)\n",
    "plt.ylim(0, 6)\n",
    "plt.xticks([0, 20, 50, 100])\n",
    "plt.xlabel(\"Top-k Removed\")\n",
    "plt.ylabel(\"Average Loss after Retraining\")\n",
    "plt.title(\"Average Loss vs Top-k Removed\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('retraining_figure.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9803428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Style settings\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 9,\n",
    "    \"font.family\": \"times new roman\",\n",
    "    \"axes.labelsize\": 9,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.dpi\": 300\n",
    "})\n",
    "\n",
    "# Plot dimensions\n",
    "fig_width = 3.25  # inches for one column\n",
    "fig_height = fig_width * 0.8\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "# Data\n",
    "x_vals = [0, 20, 50, 100]\n",
    "plt.plot(x_vals, avgs_gecko, marker='^', label='GECKO')\n",
    "plt.plot(x_vals, avgs_bm25, marker='s', label='BM25')\n",
    "plt.plot(x_vals, avgs_attributed, marker='o', label='DABGO')\n",
    "plt.plot(x_vals, avgs_trackstar, marker='^', label='TRACKSTAR')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Top-k Removed\")\n",
    "plt.ylabel(\"Loss After Retraining\")\n",
    "#plt.title(\"Loss After Retraining Without Top-k\", pad=10)\n",
    "\n",
    "\n",
    "# Legend inside plot, top-centered\n",
    "plt.legend(\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, 1),\n",
    "    ncol=3,\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    framealpha=0.9,\n",
    "    edgecolor='black',\n",
    "    borderpad=0.6\n",
    ")\n",
    "\n",
    "# Axis settings\n",
    "plt.xlim(0, 110)\n",
    "plt.ylim(0, 6)\n",
    "plt.xticks([0, 20, 50, 100])\n",
    "plt.yticks(np.arange(0, 6,1))  # or even step=0.25 if clarity improves\n",
    "\n",
    "# Layout and save\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(\"retrained_losses.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd9059",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92580a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_token_losses(model, input_ids, device, pad_token_id):\n",
    "    \"\"\"\n",
    "    Compute per-token losses (negative log-likelihoods) for a single input sequence.\n",
    "    Returns a 1D tensor of length (seq_len - 1), excluding the first token.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)  # shape: (1, seq_len)\n",
    "\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: (1, seq_len, vocab_size)\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = input_ids[:, 1:]\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        per_token_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        per_token_nll = -per_token_log_probs  # shape: (1, seq_len - 1)\n",
    "    return per_token_nll.squeeze(0)  # shape: (seq_len - 1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "model_ft = GPT2LMHeadModel.from_pretrained('out/wiki_model')\n",
    "ckpt_ft = torch.load('out/wiki_models_finetuned/fisher_regularized_models/ww2_finetuned_fisher.pt')\n",
    "ckpt_unl = torch.load('out/wiki_models_unlearned/fisher_regularized_models/ww2_unlearned_fisher.pt')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model_ft.load_state_dict(ckpt_ft['model'])\n",
    "model_ft.to('cuda')\n",
    "model_ft.eval()\n",
    "model_unl = GPT2LMHeadModel.from_pretrained('out/wiki_model')\n",
    "model_unl.load_state_dict(ckpt_unl['model'])\n",
    "model_unl.to('cuda')\n",
    "model_unl.eval()\n",
    "print('Model loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Japan's attack on Pearl Harbor took place on December 7, 1941. The U.S. military suffered 18 ships damaged or sunk, and 2,400 people were killed. Its most significant consequence was the entrance of the United States into World War II. The US had previously been neutral but subsequently entered the Pacific War, the Battle of the Atlantic and the European theatre of war. Following the attack, the US interned 120,000 Japanese Americans, 11,000 German Americans, and 3,000 Italian Americans.\n",
    "\"\"\"\n",
    "sample_ids = tokenizer.encode(sample_text, return_tensors='pt')\n",
    "prompt_length = ckpt_ft['prompt_length']\n",
    "losses_ft = get_token_losses(model_ft, sample_ids, 'cuda', tokenizer.pad_token_id)\n",
    "losses_unl = get_token_losses(model_unl, sample_ids, 'cuda', tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 30 highlight macros from strong to light blue\n",
    "hl_macros = [\n",
    "    f'\\\\hl{chr(c)}' for c in range(ord('A'), ord('Z') + 1)\n",
    "] + ['\\\\hlAA', '\\\\hlAB', '\\\\hlAC', '\\\\hlAD']  # Total = 30\n",
    "\n",
    "# Compute per-token loss difference\n",
    "token_diff = np.abs(losses_ft.cpu().numpy() - losses_unl.cpu().numpy())\n",
    "input_ids = sample_ids.squeeze(0)\n",
    "# Get top 30 most changed tokens\n",
    "top_token_indices = token_diff.argsort()[::-1][:25]\n",
    "\n",
    "# Create mapping from token index (+1 offset) to LaTeX macro\n",
    "highlight_rank = {i + 1: hl_macros[rank] for rank, i in enumerate(top_token_indices)}\n",
    "\n",
    "# Highlighted token sequence\n",
    "highlighted_tokens = []\n",
    "for i, tid in enumerate(input_ids):\n",
    "    token_str = tokenizer.decode([tid]).replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "    if tid == tokenizer.pad_token_id:\n",
    "        continue\n",
    "    if i in highlight_rank:\n",
    "        macro = highlight_rank[i]\n",
    "        highlighted_tokens.append(f\"{macro}{{{token_str}}}\")\n",
    "    else:\n",
    "        highlighted_tokens.append(token_str)\n",
    "\n",
    "# Output LaTeX-friendly string\n",
    "highlighted_sentence = \"\".join(highlighted_tokens)\n",
    "\n",
    "print(highlighted_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
