{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c61cc5c",
   "metadata": {},
   "source": [
    "## Preprocess all gutenberg books and store a selected subset with some famous authors and some random authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "df = pd.read_csv('gutenberg_books.csv')\n",
    "\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(df.iloc[0])\n",
    "\n",
    "eng_df = df[df['Language'] == 'en']\n",
    "eng_df = eng_df.drop(columns=['Link', 'Language'])\n",
    "print(eng_df.shape)\n",
    "print(eng_df.head(2))\n",
    "\n",
    "eng_df['Author'] = eng_df['Author'].astype(str)\n",
    "eng_df['Text'] = eng_df['Text'].astype(str)\n",
    "eng_df['Title'] = eng_df['Title'].astype(str)\n",
    "eng_df = eng_df.drop_duplicates(subset=['Author', 'Title'], keep='first')\n",
    "print(eng_df.shape)\n",
    "eng_df = eng_df[\n",
    "    ~eng_df['Author'].str.contains(\"United States\", case=False, na=False) &  # Remove any author containing 'United States'\n",
    "    ~eng_df['Author'].isin(['Unknown', 'Anonymous', '', 'Unknown Author', 'Unknown Authors', 'Various', 'Various Authors', 'Anonymous Authors'])\n",
    "    & ~eng_df['Author'].str.contains(\"Library of Congress\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"University\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Government\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"State\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Press\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Society\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Association\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Institute\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Commission\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Board\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Foundation\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Center\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Council\", case=False, na=False)\n",
    "    & ~eng_df['Author'].str.contains(\"Institute\", case=False, na=False)\n",
    "]\n",
    "print(eng_df.shape)\n",
    "eng_df = eng_df[~eng_df['Title'].str.contains(\"Works of\", case=False, na=False)]\n",
    "eng_df = eng_df[~eng_df['Title'].str.contains(\"Volume\", case=False, na=False)]\n",
    "eng_df = eng_df[~eng_df['Title'].str.contains(\"Vol\", case=False, na=False)]\n",
    "eng_df = eng_df[~eng_df['Title'].str.contains(\"Part\", case=False, na=False)]\n",
    "\n",
    "print(eng_df.shape)\n",
    "\n",
    "\n",
    "## Get middle of the text\n",
    "print('get middle of the text')\n",
    "print('removing the first and last parts \\n cleaning the text but keeping punctuation and stopwords etc \\n removing the first 10% of the text and the last 10% of the text to capture the middle')\n",
    "import re\n",
    "def reorder_author_name(author):\n",
    "    author = author.strip().rstrip(',')\n",
    "    if ',' in author:\n",
    "        # Split by the comma, swap the order, and remove leading/trailing spaces\n",
    "        parts = [part.strip() for part in author.split(',', 1)]\n",
    "        return f\"{parts[1]} {parts[0]}\"\n",
    "    return author.strip()\n",
    "eng_df.loc[:, 'Author'] = eng_df['Author'].astype(str)\n",
    "eng_df['Author'] = eng_df['Author'].apply(reorder_author_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_authors = ['William Shakespeare', 'Jane Austen', 'Ernest Hemingway', 'Charles Dickens', \n",
    "                    'F. Scott Fitzgerald', 'Mark Twain', 'Oscar Wilde', 'Edgar Allan Poe', 'Mary Shelley', 'George Orwell', \n",
    "                    'Virginia Woolf', 'Miguel de Cervantes', 'Herman Melville', 'J.R.R Tolkien', 'Howard Pyle']\n",
    "william_df = eng_df[eng_df['Author'] == 'William Shakespeare']\n",
    "jane_austen_df = eng_df[eng_df['Author'] == 'Jane Austen']\n",
    "ernest_hemmingway_df = eng_df[eng_df['Author'] == 'Ernest Hemingway']\n",
    "charles_dickens_df = eng_df[eng_df['Author'] == 'Charles Dickens']\n",
    "rest_df = eng_df[~eng_df['Author'].isin(special_authors)]\n",
    "selected_authors = np.random.choice(eng_df['Author'].unique(), size=10, replace=False)\n",
    "selected_authors_df = rest_df[rest_df['Author'].isin(selected_authors)]\n",
    "special_authors_df = eng_df[eng_df['Author'].isin(special_authors)]\n",
    "\n",
    "print(william_df.shape)\n",
    "print(jane_austen_df.shape)\n",
    "print(ernest_hemmingway_df.shape)\n",
    "print(charles_dickens_df.shape)\n",
    "print(selected_authors_df.shape)\n",
    "print(special_authors_df.shape)\n",
    "special_authors_df['Author'].value_counts()\n",
    "## Either getting only from special authors or from selected authors or selected special authors\n",
    "#total_df = pd.concat([william_df.iloc[:10], jane_austen_df, ernest_hemmingway_df.iloc[:10], charles_dickens_df.iloc[:10]])\n",
    "total_df = special_authors_df\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Title'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Text'])\n",
    "print(total_df.shape)\n",
    "\n",
    "\n",
    "print(total_df.loc[total_df['Author'] == 'Edgar Allan Poe', 'Title'].tolist())\n",
    "total_df = total_df.groupby('Author').head(3).reset_index(drop=True)\n",
    "total_df = total_df[total_df['Title'] != 'First Project Gutenberg Collection of Edgar Allan Poe']\n",
    "total_df.loc[:, ['Author', 'Title']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_main_text(text):\n",
    "    # Find the start and end markers\n",
    "    start_match = re.search(r\"\\*\\*\\* *START OF THE PROJECT GUTENBERG.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "    end_match = re.search(r\"\\*\\*\\* *END OF THE PROJECT GUTENBERG.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "\n",
    "    # Extract the main content if both markers are found\n",
    "    if start_match and end_match:\n",
    "        start_idx = start_match.end()\n",
    "        end_idx = end_match.start()\n",
    "        text = text[start_idx:end_idx].strip()\n",
    "    text = re.sub(r'(Produced by.*?)(\\n|$)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'End of the Project Gutenberg.*?(\\n|$)', '', text, flags=re.DOTALL)\n",
    "    text = text.strip()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove text inside (), [], and {}\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', text)\n",
    "    text = text.replace('_', ' ')              # <— strip underscores\n",
    "    text = re.sub(r'-{2,}', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\'\\\"\\;\\:\\n]', '', text)  # keep common punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    cutoff_index = int(len(words) * 0.1)\n",
    "    \n",
    "    return ' '.join(words[cutoff_index:-cutoff_index]).strip()\n",
    "\n",
    "total_df = total_df[~total_df['Title'].str.contains(r'^works of', case=False, na=False)]\n",
    "total_df = total_df[~total_df['Title'].str.contains(r'volumes', case=False, na=False)]\n",
    "total_df = total_df.drop_duplicates(subset=['Title'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df[~total_df['Title'].str.contains(\"United States\", case=False, na=False)]\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Author', 'Title'])\n",
    "# Apply the function to the 'Text' column\n",
    "total_df['Text'] = total_df['Text'].apply(extract_main_text)\n",
    "total_df = total_df.drop_duplicates(subset=['Text'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df.dropna(subset=['Text', 'Author'])\n",
    "print(total_df.shape)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<SEP>\"], \"pad_token\": \"<PAD>\"})\n",
    "\n",
    "# Create dataset\n",
    "\n",
    "\n",
    "import re\n",
    "def middle_of_text_by_sentence(text):\n",
    "    # Split the text into sentences using punctuation as a delimiter\n",
    "    sentences = re.split(r'(?<=[.])\\s+', text.strip())\n",
    "    # Remove first and last 20% of sentences\n",
    "    n = len(sentences)\n",
    "    start = int(0.3 * n)\n",
    "    end = int(0.7 * n)\n",
    "    middle_sentences = sentences[start:end]\n",
    "    return ' '.join(middle_sentences).strip()\n",
    "total_df['Text_'] = total_df.loc[:, 'Text'].apply(middle_of_text_by_sentence)\n",
    "total_df = total_df.drop(columns=['Text'])\n",
    "df_ = total_df.rename(columns={'Text_': 'Text'})\n",
    "df_['book_length'] = df_['Text'].apply(len)\n",
    "\n",
    "df_.head(4)\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "block_size = 128  # GPT-style block size\n",
    "step_size = block_size//2\n",
    "def tokenize_and_chunk(row, tokenizer=tokenizer, block_size=block_size, step_size=step_size):\n",
    "    author = row['Author']\n",
    "    text = row['Text']\n",
    "    title = row['Title']\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "    chunks = [(tokens[i:i+block_size], author, title) for i in range(0, len(tokens) - block_size, step_size)]\n",
    "\n",
    "    return chunks\n",
    "print('tokenizing and chunking...')\n",
    "author_text = df_.loc[:, ['Author', 'Text', 'Title']].apply(tokenize_and_chunk, axis=1)\n",
    "author_text_list=author_text.tolist()\n",
    "print(step_size)\n",
    "total_tokens = sum([len(chunk[0]) for chunk_ in author_text_list for chunk in chunk_])\n",
    "print(total_tokens)\n",
    "\n",
    "df_.head(40)\n",
    "#df_.to_csv('data/datasets/selected_dataset_small.csv', index=False)\n",
    "## Either getting only from special authors or from selected authors or selected special authors\n",
    "#total_df = pd.concat([william_df.iloc[:10], jane_austen_df, ernest_hemmingway_df.iloc[:10], charles_dickens_df.iloc[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_df = eng_df.sample(20, random_state=46)\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Title'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Text'])\n",
    "print(total_df.shape)\n",
    "total_df.head(25)\n",
    "\n",
    "def extract_main_text(text):\n",
    "    # Find the start and end markers\n",
    "    start_match = re.search(r\"\\*\\*\\* *START OF THE PROJECT GUTENBERG.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "    end_match = re.search(r\"\\*\\*\\* *END OF THE PROJECT GUTENBERG.*?\\*\\*\\*\", text, re.IGNORECASE)\n",
    "\n",
    "    # Extract the main content if both markers are found\n",
    "    if start_match and end_match:\n",
    "        start_idx = start_match.end()\n",
    "        end_idx = end_match.start()\n",
    "        text = text[start_idx:end_idx].strip()\n",
    "    text = re.sub(r'(Produced by.*?)(\\n|$)', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'End of the Project Gutenberg.*?(\\n|$)', '', text, flags=re.DOTALL)\n",
    "    text = text.strip()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove text inside (), [], and {}\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\{.*?\\}', '', text)\n",
    "    text = text.replace('_', ' ')              # <— strip underscores\n",
    "    text = re.sub(r'-{2,}', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\'\\\"\\;\\:\\n]', '', text)  # keep common punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    # Calculate the 10% cutoff\n",
    "    cutoff_index = int(len(words) * 0.1)\n",
    "    \n",
    "    return ' '.join(words[cutoff_index:-cutoff_index]).strip()\n",
    "\n",
    "total_df = total_df[~total_df['Title'].str.contains(r'^works of', case=False, na=False)]\n",
    "total_df = total_df[~total_df['Title'].str.contains(r'volumes', case=False, na=False)]\n",
    "total_df = total_df.drop_duplicates(subset=['Title'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df[~total_df['Title'].str.contains(\"United States\", case=False, na=False)]\n",
    "print(total_df.shape)\n",
    "total_df = total_df.drop_duplicates(subset=['Author', 'Title'])\n",
    "# Apply the function to the 'Text' column\n",
    "total_df['Text'] = total_df['Text'].apply(extract_main_text)\n",
    "total_df = total_df.drop_duplicates(subset=['Text'])\n",
    "print(total_df.shape)\n",
    "total_df = total_df.dropna(subset=['Text', 'Author'])\n",
    "print(total_df.shape)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset\n",
    "\n",
    "\n",
    "import re\n",
    "def middle_of_text_by_sentence(text):\n",
    "    # Split the text into sentences using punctuation as a delimiter\n",
    "    sentences = re.split(r'(?<=[.])\\s+', text.strip())\n",
    "    # Remove first and last 20% of sentences\n",
    "    n = len(sentences)\n",
    "    start = int(0.3 * n)\n",
    "    end = int(0.7 * n)\n",
    "    middle_sentences = sentences[start:end]\n",
    "    return ' '.join(middle_sentences).strip()\n",
    "total_df['Text_'] = total_df.loc[:, 'Text'].apply(middle_of_text_by_sentence)\n",
    "total_df = total_df.drop(columns=['Text'])\n",
    "df_random = total_df.rename(columns={'Text_': 'Text'})\n",
    "df_random['book_length'] = df_random['Text'].apply(len)\n",
    "\n",
    "df_random = df_random[df_random['Author'] != 'William Chaffers']\n",
    "df_random = df_random[df_random['Author'] != 'Morris Jastrow']\n",
    "df_random.head(25)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # required for compatibility\n",
    "block_size = 128  # GPT-style block size\n",
    "step_size = block_size//2\n",
    "def tokenize_and_chunk(row, tokenizer=tokenizer, block_size=block_size, step_size=step_size):\n",
    "    author = row['Author']\n",
    "    text = row['Text']\n",
    "    title = row['Title']\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "    chunks = [(tokens[i:i+block_size], author, title) for i in range(0, len(tokens) - block_size, step_size)]\n",
    "\n",
    "    return chunks\n",
    "print('tokenizing and chunking...')\n",
    "author_text = df_random.loc[:, ['Author', 'Text', 'Title']].apply(tokenize_and_chunk, axis=1)\n",
    "author_text_list=author_text.tolist()\n",
    "print(step_size)\n",
    "total_tokens = sum([len(chunk[0]) for chunk_ in author_text_list for chunk in chunk_])\n",
    "print(total_tokens)\n",
    "combined_df = pd.concat([df_, df_random])\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # required for compatibility\n",
    "block_size = 128  # GPT-style block size\n",
    "step_size = block_size//2\n",
    "def tokenize_and_chunk(row, tokenizer=tokenizer, block_size=block_size, step_size=step_size):\n",
    "    author = row['Author']\n",
    "    text = row['Text']\n",
    "    title = row['Title']\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "    chunks = [(tokens[i:i+block_size], author, title) for i in range(0, len(tokens) - block_size, step_size)]\n",
    "\n",
    "    return chunks\n",
    "print('tokenizing and chunking...')\n",
    "author_text = combined_df.loc[:, ['Author', 'Text', 'Title']].apply(tokenize_and_chunk, axis=1)\n",
    "author_text_list=author_text.tolist()\n",
    "print(step_size)\n",
    "total_tokens = sum([len(chunk[0]) for chunk_ in author_text_list for chunk in chunk_])\n",
    "print(total_tokens)\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df.head(45)\n",
    "combined_df.to_csv('selected_dataset_mixed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.1, random_state=42)\n",
    "def tokenize_and_chunk(row, tokenizer=tokenizer, block_size=block_size, step_size=step_size):\n",
    "    author = row['Author']\n",
    "    text = row['Text']\n",
    "    title = row['Title']\n",
    "    text_chunks = []\n",
    "    title_chunks = []\n",
    "    author_chunks = []\n",
    "    for i in range(0, len(text) - block_size, step_size):\n",
    "        text_chunks.append(text[i:i+block_size])\n",
    "        title_chunks.append(title)\n",
    "        author_chunks.append(author)\n",
    "    return text_chunks, title_chunks, author_chunks\n",
    "\n",
    "train_data_np, train_data_titles, train_data_authors = train_df.apply(tokenize_and_chunk, axis=1)\n",
    "eval_data_np, eval_data_titles, eval_data_authors = test_df.apply(tokenize_and_chunk, axis=1)\n",
    "selected_dataset = {\n",
    "    'train_data_np': train_data_np,\n",
    "    'train_data_titles': train_data_titles,\n",
    "    'train_data_authors': train_data_authors,\n",
    "    'eval_data_np': eval_data_np,\n",
    "    'eval_data_titles': eval_data_titles,\n",
    "    'eval_data_authors': eval_data_authors\n",
    "}\n",
    "import json\n",
    "with open('selected_dataset_mixed.json', 'w') as f:\n",
    "    json.dump(selected_dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('selected_dataset_mixed.csv')\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a539581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
