{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ccb635",
   "metadata": {},
   "source": [
    "## Token Losses for Interpretability\n",
    "* Pass in relevant data samples and compute which tokens had a big loss deviation between ascent and descent optimized models. Highlighted output can be seen in appendix of paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e897cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_token_losses(model, input_ids, device, pad_token_id):\n",
    "    \"\"\"\n",
    "    Compute per-token losses (negative log-likelihoods) for a single input sequence.\n",
    "    Returns a 1D tensor of length (seq_len - 1), excluding the first token.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = input_ids.to(device)  # shape: (1, seq_len)\n",
    "\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: (1, seq_len, vocab_size)\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :]\n",
    "        shift_labels = input_ids[:, 1:]\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        per_token_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        per_token_nll = -per_token_log_probs  # shape: (1, seq_len - 1)\n",
    "    return per_token_nll.squeeze(0)  # shape: (seq_len - 1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_ft = GPT2LMHeadModel.from_pretrained('../out/wiki_model')\n",
    "ckpt_ft = torch.load('../out/wiki_models_finetuned/fisher_regularized_models/ancient_rome_finetuned_fisher.pt')\n",
    "ckpt_unl = torch.load('../out/wiki_models_unlearned/fisher_regularized_models/ancient_rome_unlearned_fisher.pt')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model_ft.load_state_dict(ckpt_ft['model'])\n",
    "model_ft.to('cuda')\n",
    "model_ft.eval()\n",
    "model_unl = GPT2LMHeadModel.from_pretrained('../out/wiki_model')\n",
    "model_unl.load_state_dict(ckpt_unl['model'])\n",
    "model_unl.to('cuda')\n",
    "model_unl.eval()\n",
    "print('Model loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4257091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "The following outline is provided as an overview of and topical guide to ancient Rome: Ancient Rome â€“ former civilization that thrived on the Italian Peninsula as early as the 8th century BC. Located along the Mediterranean Sea and\n",
    "centered on the city of Rome, it expanded to become one of the largest empires in the ancient world\"\"\"\n",
    "sample_ids = tokenizer.encode(sample_text, return_tensors='pt')\n",
    "prompt_length = ckpt_ft['prompt_length']\n",
    "losses_ft = get_token_losses(model_ft, sample_ids, 'cuda', tokenizer.pad_token_id)\n",
    "losses_unl = get_token_losses(model_unl, sample_ids, 'cuda', tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb17d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 30 highlight macros from strong to light blue\n",
    "hl_macros = [\n",
    "    f'\\\\hl{chr(c)}' for c in range(ord('A'), ord('Z') + 1)\n",
    "] + ['\\\\hlAA', '\\\\hlAB', '\\\\hlAC', '\\\\hlAD']  # Total = 30\n",
    "\n",
    "# Compute per-token loss difference\n",
    "token_diff = np.abs(losses_ft.cpu().numpy() - losses_unl.cpu().numpy())\n",
    "input_ids = sample_ids.squeeze(0)\n",
    "# Get top 30 most changed tokens\n",
    "top_token_indices = token_diff.argsort()[::-1][:25]\n",
    "\n",
    "# Create mapping from token index (+1 offset) to LaTeX macro\n",
    "highlight_rank = {i + 1: hl_macros[rank] for rank, i in enumerate(top_token_indices)}\n",
    "\n",
    "# Highlighted token sequence\n",
    "highlighted_tokens = []\n",
    "for i, tid in enumerate(input_ids):\n",
    "    token_str = tokenizer.decode([tid]).replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n",
    "    if tid == tokenizer.pad_token_id:\n",
    "        continue\n",
    "    if i in highlight_rank:\n",
    "        macro = highlight_rank[i]\n",
    "        highlighted_tokens.append(f\"{macro}{{{token_str}}}\")\n",
    "    else:\n",
    "        highlighted_tokens.append(token_str)\n",
    "\n",
    "# Output LaTeX-friendly string\n",
    "highlighted_sentence = \"\".join(highlighted_tokens)\n",
    "\n",
    "print(highlighted_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
